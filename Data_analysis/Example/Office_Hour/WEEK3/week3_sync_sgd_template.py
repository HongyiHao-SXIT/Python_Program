# -*- coding: utf-8 -*-
"""Week3 - Sync sgd.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wcdTUL7c4gUYxVl323H8AOgnzzygbosA
"""

import numpy as np
import matplotlib.pyplot as plt
from multiprocessing import Pool

# Define the polynomial function for the loss
def polynomial_loss(theta, X, y):
    m = len(y)  # Number of training examples
    h = np.dot(X, theta)  # Hypothesis function
    loss = np.sum((h - y) ** 2) / (2 * m)  # Polynomial loss function
    return loss

# Function to plot the contour of the loss function
def plot_loss_contour(X, y, theta_history, theta_range=(-10, 10)):
    # Create a meshgrid for theta0 and theta1 values
    theta0_vals = np.linspace(theta_range[0], theta_range[1], 100)
    theta1_vals = np.linspace(theta_range[0], theta_range[1], 100)
    theta0_mesh, theta1_mesh = np.meshgrid(theta0_vals, theta1_vals)

    # Compute the loss for each combination of theta0 and theta1
    loss_vals = np.zeros_like(theta0_mesh)
    for i in range(len(theta0_vals)):
        for j in range(len(theta1_vals)):
            theta = np.array([theta0_mesh[i, j], theta1_mesh[i, j]])
            loss_vals[i, j] = polynomial_loss(theta, X, y)

    # Plot the contour of the loss function
    plt.figure(figsize=(8, 6))
    plt.contour(theta0_mesh, theta1_mesh, loss_vals, levels=20, cmap='jet')

    # Plot the theta values
    for i in range(len(theta_history) - 1):
        plt.plot(theta_history[i][0], theta_history[i][1], 'bo-', alpha=0.4, linewidth=.5, markersize=2)
    plt.plot(theta_history[-1][0], theta_history[-1][1], 'ro-', alpha=0.8, linewidth=.5, markersize=2)

    plt.xlim(-10, 10)
    plt.ylim(-10, 10)
    plt.xlabel('theta0')
    plt.ylabel('theta1')
    plt.title('Contour of Loss Function')
    plt.colorbar()
    plt.show()

# Function to perform stochastic gradient descent for a batch
def stochastic_gradient_descent(args):

    X, y, theta = args

    m = len(y)  # Number of training examples
    h = ?  # Hypothesis function
    error = ?  # Error
    gradient = ?  #TODO: Calculate the gradient

    return gradient

# Perform synchronized distributed stochastic gradient descent
def synchronized_distributed_sgd(X, y, batch_size, learning_rate, num_iterations, num_workers):
    m, n = X.shape  # Number of training examples and features
    theta = np.zeros(n)  # Initialize the parameters
    theta_history = [theta.copy()]  # Store the intermediate theta values

    for iter in range(num_iterations):

        with Pool(processes=num_workers) as pool:

            # Prepare arguments for parallel execution
            args_list = []
            for _ in range(num_workers):
                index = np.random.choice(len(X), batch_size // num_workers)
                X_batch = X[index, :] #mini-BatchSize
                y_batch = y[index]
                args_list.append((X_batch, y_batch, theta))

            # Perform parallel stochastic gradient descent for each batch
            grad_list = pool.map(stochastic_gradient_descent, args_list)

            # Average the updated parameters from all workers
            theta = ? #TODO: Perform gradient descent with the gradient average.

            theta_history.append(theta.copy())  # Save the intermediate theta values

        # Print the loss for every 10 epochs
        if (iter + 1) % 10 == 0:
            loss = polynomial_loss(theta, X, y)
            print(f"Epoch {iter+1}: Loss = {loss:.4f}")

    return theta, theta_history

def main():
    # Generate synthetic data
    np.random.seed(0)
    X = np.random.rand(1000, 2)  # 100 random 2-dimensional points
    y = 7 * X[:, 0] + 1 * X[:, 1] + 1*np.random.randn(1000)  # Linear relationship with some random noise

    # Set hyperparameters
    batch_size = ?
    learning_rate = ?
    num_iterations = ?
    num_workers = ?  # Number of distributed workers

    # Run synchronized distributed SGD
    theta, theta_history = synchronized_distributed_sgd(X, y, batch_size, learning_rate, num_iterations, num_workers)

    # Plot the loss contour and theta values
    plot_loss_contour(X, y, theta_history)

if __name__ == '__main__':
    main()
